{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/codebase-concept-mapper/"
      ],
      "metadata": {
        "id": "fJCNvs0DBdlG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup HuggingFace Token\n",
        "# Go to: https://huggingface.co/settings/tokens\n",
        "# Create a token, then add it to Colab secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')  # Add via left sidebar â†’ ğŸ”‘ Secrets\n",
        "login(token=hf_token)\n",
        "\n",
        "# Also request access to gated model:\n",
        "# Visit: https://huggingface.co/google/embeddinggemma-300m\n",
        "# Click \"Request Access\" button"
      ],
      "metadata": {
        "id": "gnjX3xOyM3SN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YKqgbz9fppG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree -L 2 /content/codebase-concept-mapper -I '__pycache__|*.pyc|.git|test_code|temp_repo'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plr_ko8ghvam",
        "outputId": "6b9e30bb-cb10-4a16-90e3-a41db26ba9a3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/codebase-concept-mapper\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mDEV\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mEvaluate_Embedding_Models_on_Code_Concepts.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_diagnostic.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;34mphase1_results\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mphase1_root_cause_analysis.md\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mdocs\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mCONTEXT.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_context.md\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mPHASED_PLAN.md\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mphase1\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mall_results.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mcomparison.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mconcept_validators.py\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mdataset_generator.py\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mMODEL_RECOMMENDATIONS.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_decision.txt\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mproject_structure.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mQUICKSTART.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mREADME_PHASE1.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresearch_query.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_Alibaba-NLP_gte-multilingual-base.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_BAAI_bge-small-en-v1.5.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_google_embeddinggemma-300m.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_intfloat_multilingual-e5-large-instruct.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_nomic-ai_CodeRankEmbed.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_nomic-ai_nomic-embed-text-v1.5.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_Qwen_Qwen3-Embedding-0.6B.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;32mrun_phase1.sh\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;32msetup_phase1.sh\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;34mtemp_repos\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mtest_code_specific_quick.py\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mtest_embeddings.py\u001b[0m\n",
            "â””â”€â”€ \u001b[00mrequirements.txt\u001b[0m\n",
            "\n",
            "5 directories, 28 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Clear GPU Memory\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_properties(0).name}\")\n",
        "print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYX3IEQ4M-4L",
        "outputId": "4aeea9a9-7e78-4494-e788-83a2e2ccdd13"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Total Memory: 15.83 GB\n",
            "Available Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KAyes3N8ATDX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Suppress installation output\n",
        "!pip install sentence-transformers torch numpy matplotlib\n",
        "\n",
        "# Mount Google Drive for persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "print(\"âœ“ Dependencies installed\")\n",
        "print(\"âœ“ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "REPO_URL = \"https://github.com/akbargherbal/codebase-concept-mapper.git\"\n",
        "REPO_NAME = \"codebase-concept-mapper\"\n",
        "WORKING_DIR = f\"/content/{REPO_NAME}/phase1\"\n",
        "\n",
        "def setup_repository():\n",
        "    \"\"\"Clone or sync the repository and navigate to working directory.\"\"\"\n",
        "\n",
        "    repo_path = Path(f\"/content/{REPO_NAME}\")\n",
        "\n",
        "    # Check if repository already exists\n",
        "    if repo_path.exists():\n",
        "        print(f\"âœ“ Repository '{REPO_NAME}' already exists\")\n",
        "\n",
        "        # Check if it's corrupted (common issue)\n",
        "        try:\n",
        "            os.chdir(repo_path)\n",
        "            print(\"\\nâ†’ Syncing with remote...\")\n",
        "\n",
        "            # Fetch latest changes\n",
        "            !git fetch origin\n",
        "\n",
        "            # Show current branch and status\n",
        "            !git branch --show-current\n",
        "            !git status --short\n",
        "\n",
        "            # Pull latest changes (assuming main branch, adjust if needed)\n",
        "            print(\"\\nâ†’ Pulling latest changes...\")\n",
        "            !git pull origin main\n",
        "\n",
        "        except (FileNotFoundError, OSError) as e:\n",
        "            print(f\"\\nâš ï¸ Repository corrupted: {e}\")\n",
        "            print(\"ğŸ—‘ï¸ Removing and re-cloning...\")\n",
        "\n",
        "            # Go to safe directory first\n",
        "            os.chdir('/content')\n",
        "\n",
        "            # Force remove corrupted directory\n",
        "            shutil.rmtree(repo_path, ignore_errors=True)\n",
        "\n",
        "            # Clone fresh\n",
        "            print(f\"\\nâ†’ Cloning repository '{REPO_NAME}'...\")\n",
        "            !git clone {REPO_URL}\n",
        "            print(f\"âœ“ Repository cloned successfully\")\n",
        "\n",
        "    else:\n",
        "        print(f\"â†’ Cloning repository '{REPO_NAME}'...\")\n",
        "        os.chdir('/content')  # Make sure we're in a valid directory\n",
        "        !git clone {REPO_URL}\n",
        "        print(f\"âœ“ Repository cloned successfully\")\n",
        "\n",
        "    # Verify structure\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"REPOSITORY STRUCTURE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Check if tree command exists, otherwise use ls\n",
        "    tree_check = !which tree\n",
        "    if tree_check:\n",
        "        !tree -L 2 {repo_path} -I '__pycache__|*.pyc|.git'\n",
        "    else:\n",
        "        print(\"(tree not installed, using ls)\")\n",
        "        !ls -la {repo_path}\n",
        "        if Path(f\"{repo_path}/phase1\").exists():\n",
        "            print(f\"\\nContents of phase1/:\")\n",
        "            !ls -la {repo_path}/phase1\n",
        "\n",
        "    # Navigate to working directory\n",
        "    working_path = Path(WORKING_DIR)\n",
        "    if working_path.exists():\n",
        "        os.chdir(WORKING_DIR)\n",
        "        print(f\"\\nâœ“ Changed to working directory: {WORKING_DIR}\")\n",
        "        print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "        # Show what's in the working directory\n",
        "        print(\"\\nFiles in working directory:\")\n",
        "        !ls -lh\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Warning: '{WORKING_DIR}' does not exist\")\n",
        "\n",
        "        # Try to navigate to repo root at least\n",
        "        try:\n",
        "            os.chdir(repo_path)\n",
        "            print(f\"Staying in repo root: {os.getcwd()}\")\n",
        "            print(\"\\nAvailable directories:\")\n",
        "            !ls -la\n",
        "        except:\n",
        "            print(f\"Staying in: /content\")\n",
        "            os.chdir('/content')\n",
        "\n",
        "# Run setup\n",
        "setup_repository()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViTQeJdDAa9_",
        "outputId": "21aada8a-290c-478d-82d4-3c6717199f10"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Repository 'codebase-concept-mapper' already exists\n",
            "\n",
            "â†’ Syncing with remote...\n",
            "main\n",
            " \u001b[31mM\u001b[m phase1/all_results.json\n",
            " \u001b[31mM\u001b[m phase1/comparison.md\n",
            " \u001b[31mM\u001b[m phase1/results_nomic-ai_CodeRankEmbed.json\n",
            " \u001b[31mM\u001b[m phase1/results_nomic-ai_nomic-embed-text-v1.5.json\n",
            "\u001b[31m??\u001b[m phase1/phase1_decision.txt\n",
            "\u001b[31m??\u001b[m phase1/results_Alibaba-NLP_gte-multilingual-base.json\n",
            "\u001b[31m??\u001b[m phase1/results_Qwen_Qwen3-Embedding-0.6B.json\n",
            "\u001b[31m??\u001b[m phase1/results_google_embeddinggemma-300m.json\n",
            "\u001b[31m??\u001b[m phase1/results_intfloat_multilingual-e5-large-instruct.json\n",
            "\n",
            "â†’ Pulling latest changes...\n",
            "From https://github.com/akbargherbal/codebase-concept-mapper\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "\n",
            "==================================================\n",
            "REPOSITORY STRUCTURE\n",
            "==================================================\n",
            "\u001b[01;34m/content/codebase-concept-mapper\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mDEV\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mEvaluate_Embedding_Models_on_Code_Concepts.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_diagnostic.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;34mphase1_results\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mphase1_root_cause_analysis.md\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mdocs\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mCONTEXT.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_context.md\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mPHASED_PLAN.md\u001b[0m\n",
            "â”œâ”€â”€ \u001b[01;34mphase1\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mall_results.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mcomparison.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mconcept_validators.py\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mdataset_generator.py\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mMODEL_RECOMMENDATIONS.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mphase1_decision.txt\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mproject_structure.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mQUICKSTART.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mREADME_PHASE1.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresearch_query.md\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_Alibaba-NLP_gte-multilingual-base.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_BAAI_bge-small-en-v1.5.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_google_embeddinggemma-300m.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_intfloat_multilingual-e5-large-instruct.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_nomic-ai_CodeRankEmbed.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_nomic-ai_nomic-embed-text-v1.5.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mresults_Qwen_Qwen3-Embedding-0.6B.json\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;32mrun_phase1.sh\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;32msetup_phase1.sh\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;34mtemp_repos\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[01;34mtest_code\u001b[0m\n",
            "â”‚Â Â  â”œâ”€â”€ \u001b[00mtest_code_specific_quick.py\u001b[0m\n",
            "â”‚Â Â  â””â”€â”€ \u001b[00mtest_embeddings.py\u001b[0m\n",
            "â””â”€â”€ \u001b[00mrequirements.txt\u001b[0m\n",
            "\n",
            "6 directories, 28 files\n",
            "\n",
            "âœ“ Changed to working directory: /content/codebase-concept-mapper/phase1\n",
            "Current directory: /content/codebase-concept-mapper/phase1\n",
            "\n",
            "Files in working directory:\n",
            "total 360K\n",
            "-rw-r--r--  1 root root  99K Dec  4 08:38 all_results.json\n",
            "-rw-r--r--  1 root root  883 Dec  4 08:38 comparison.md\n",
            "-rw-r--r--  1 root root  12K Dec  4 08:33 concept_validators.py\n",
            "-rw-r--r--  1 root root  12K Dec  4 08:33 dataset_generator.py\n",
            "-rw-r--r--  1 root root 8.7K Dec  4 08:33 MODEL_RECOMMENDATIONS.md\n",
            "-rw-r--r--  1 root root   76 Dec  4 08:38 phase1_decision.txt\n",
            "-rw-r--r--  1 root root  13K Dec  4 08:33 project_structure.md\n",
            "drwxr-xr-x  2 root root 4.0K Dec  4 08:36 __pycache__\n",
            "-rw-r--r--  1 root root  11K Dec  4 08:33 QUICKSTART.md\n",
            "-rw-r--r--  1 root root 9.5K Dec  4 08:33 README_PHASE1.md\n",
            "-rw-r--r--  1 root root 7.1K Dec  4 08:33 research_query.md\n",
            "-rw-r--r--  1 root root  17K Dec  4 08:37 results_Alibaba-NLP_gte-multilingual-base.json\n",
            "-rw-r--r--  1 root root  18K Dec  4 08:33 results_BAAI_bge-small-en-v1.5.json\n",
            "-rw-r--r--  1 root root  16K Dec  4 08:38 results_google_embeddinggemma-300m.json\n",
            "-rw-r--r--  1 root root  17K Dec  4 08:36 results_intfloat_multilingual-e5-large-instruct.json\n",
            "-rw-r--r--  1 root root  529 Dec  4 08:36 results_nomic-ai_CodeRankEmbed.json\n",
            "-rw-r--r--  1 root root  17K Dec  4 08:36 results_nomic-ai_nomic-embed-text-v1.5.json\n",
            "-rw-r--r--  1 root root  18K Dec  4 08:37 results_Qwen_Qwen3-Embedding-0.6B.json\n",
            "-rwxr-xr-x  1 root root 3.6K Dec  4 08:33 run_phase1.sh\n",
            "-rwxr-xr-x  1 root root 1.5K Dec  4 08:33 setup_phase1.sh\n",
            "drwxr-xr-x 10 root root 4.0K Dec  4 08:35 temp_repos\n",
            "drwxr-xr-x  4 root root 4.0K Dec  4 08:35 test_code\n",
            "-rw-r--r--  1 root root 6.1K Dec  4 08:33 test_code_specific_quick.py\n",
            "-rw-r--r--  1 root root  20K Dec  4 08:33 test_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iR0j7ZG7A2j8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Silent install (suppress output)\n",
        "!pip install -r /content/codebase-concept-mapper/requirements.txt -q\n",
        "\n",
        "print(\"âœ“ Dependencies installed\")"
      ],
      "metadata": {
        "id": "UybUy924A5i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f0f155-d1c8-4b51-e070-f093571b6370"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dependencies installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/codebase-concept-mapper/phase1/test_code_specific_quick.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJC3QM9IhU5-",
        "outputId": "3178b22f-8718-4502-9235-fc09f06eab25"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-04 08:44:24.597244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764837864.616914    4763 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764837864.623045    4763 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764837864.638962    4763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837864.638987    4763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837864.638991    4763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837864.638997    4763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-04 08:44:24.643560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "============================================================\n",
            "QUICK VALIDATION: CodeRankEmbed\n",
            "============================================================\n",
            "Device: cuda\n",
            "\n",
            "ğŸ“¦ Loading nomic-ai/CodeRankEmbed...\n",
            "WARNING:transformers_modules.nomic_hyphen_ai.CodeRankEmbed.3c4b60807d71f79b43f3c4363786d9493691f8b1.modeling_hf_nomic_bert:<All keys matched successfully>\n",
            "âœ… Model loaded\n",
            "\n",
            "ğŸ“‚ Loading test files...\n",
            "âœ… Loaded 74 files\n",
            "\n",
            "ğŸ”„ Embedding code files...\n",
            "Batches:   0% 0/10 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/codebase-concept-mapper/phase1/test_code_specific_quick.py\", line 187, in <module>\n",
            "    quick_test_coderank()\n",
            "  File \"/content/codebase-concept-mapper/phase1/test_code_specific_quick.py\", line 82, in quick_test_coderank\n",
            "    file_embeddings = model.encode(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\", line 1094, in encode\n",
            "    out_features = self.forward(features, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\", line 1175, in forward\n",
            "    input = module(input, **module_kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\", line 261, in forward\n",
            "    outputs = self.auto_model(**trans_features, **kwargs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/nomic_hyphen_ai/CodeRankEmbed/3c4b60807d71f79b43f3c4363786d9493691f8b1/modeling_hf_nomic_bert.py\", line 1068, in forward\n",
            "    sequence_output = self.encoder(hidden_states, attention_mask=attention_mask, return_dict=return_dict)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/nomic_hyphen_ai/CodeRankEmbed/3c4b60807d71f79b43f3c4363786d9493691f8b1/modeling_hf_nomic_bert.py\", line 955, in forward\n",
            "    hidden_states, hidden_states2, residual = layer(\n",
            "                                              ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/nomic_hyphen_ai/CodeRankEmbed/3c4b60807d71f79b43f3c4363786d9493691f8b1/modeling_hf_nomic_bert.py\", line 888, in forward\n",
            "    attn_outputs = self.attn(\n",
            "                   ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/nomic_hyphen_ai/CodeRankEmbed/3c4b60807d71f79b43f3c4363786d9493691f8b1/modeling_hf_nomic_bert.py\", line 792, in forward\n",
            "    attention_scores = torch.matmul(query, key.transpose(-1, -2)) / self.norm_factor\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.03 GiB is free. Process 45628 has 8.71 GiB memory in use. Of the allocated memory 8.44 GiB is allocated by PyTorch, and 149.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected!\")\n",
        "    print(\"   Go to: Runtime > Change runtime type > Select T4 GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12V20ojzCcVr",
        "outputId": "3194a837-bfc8-4311-8187-c1a469fc39f0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPU: Tesla T4\n",
            "   Memory: 15.8 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ“ Google Drive mounted\")\n",
        "print(\"   Backups will be saved to: /content/drive/MyDrive/phase1_results_*\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3x4WW-KCeui",
        "outputId": "e3b37b83-2f87-4516-9ee6-af0f0d33e251"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Google Drive mounted\n",
            "   Backups will be saved to: /content/drive/MyDrive/phase1_results_*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Generate with Drive backup\n",
        "!python dataset_generator.py\n",
        "\n",
        "# Verify\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"test_code/metadata.json\").exists():\n",
        "    with open(\"test_code/metadata.json\") as f:\n",
        "        meta = json.load(f)\n",
        "    print(f\"\\nâœ… Dataset ready!\")\n",
        "    print(f\"   Total: {meta['stats']['total_files']} files\")\n",
        "    print(f\"   Python: {meta['stats']['python_files']}\")\n",
        "    print(f\"   JavaScript: {meta['stats']['javascript_files']}\")\n",
        "else:\n",
        "    print(\"âŒ Generation failed - check errors above\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00UkD2yVDOFb",
        "outputId": "c3ab1e9c-8f4a-48bb-d425-2f0493a059fb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PHASE 1: GENERATING TEST DATASET (COLAB OPTIMIZED)\n",
            "============================================================\n",
            "âœ“ Created output directories in test_code\n",
            "\n",
            "ğŸ“¦ Processing PYTHON repositories...\n",
            "\n",
            "  Repository: pallets/flask\n",
            "  Repository already exists at temp_repos/flask\n",
            "  Found 12 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  Repository: psf/requests\n",
            "  Repository already exists at temp_repos/requests\n",
            "  Found 9 candidate files\n",
            "  Sampled 9 files\n",
            "\n",
            "  Repository: aio-libs/aiohttp\n",
            "  Repository already exists at temp_repos/aiohttp\n",
            "  Found 28 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  Repository: django/django\n",
            "  Repository already exists at temp_repos/django\n",
            "  Found 353 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  âœ“ PYTHON: 45 files collected\n",
            "\n",
            "ğŸ“¦ Processing JAVASCRIPT repositories...\n",
            "\n",
            "  Repository: axios/axios\n",
            "  Repository already exists at temp_repos/axios\n",
            "  Found 24 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  Repository: expressjs/express\n",
            "  Repository already exists at temp_repos/express\n",
            "  Found 3 candidate files\n",
            "  Sampled 3 files\n",
            "\n",
            "  Repository: facebook/react\n",
            "  Repository already exists at temp_repos/react\n",
            "  Found 19 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  Repository: vercel/next.js\n",
            "  Repository already exists at temp_repos/next.js\n",
            "  Found 520 candidate files\n",
            "  Sampled 12 files\n",
            "\n",
            "  âœ“ JAVASCRIPT: 39 files collected\n",
            "\n",
            "============================================================\n",
            "DATASET GENERATION COMPLETE\n",
            "============================================================\n",
            "Total files: 84\n",
            "Python: 45\n",
            "JavaScript: 39\n",
            "Metadata saved to: test_code/metadata.json\n",
            "============================================================\n",
            "\n",
            "\n",
            "âœ… Dataset ready!\n",
            "   Total: 84 files\n",
            "   Python: 45\n",
            "   JavaScript: 39\n",
            "CPU times: user 9.85 ms, sys: 175 Âµs, total: 10 ms\n",
            "Wall time: 1.42 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Test candidate models\n",
        "!python test_embeddings.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spk-RPexDWOz",
        "outputId": "8cee98bc-c0ff-413d-dacb-5c2592672506"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-04 08:44:49.839162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764837889.858951    4941 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764837889.864888    4941 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764837889.879607    4941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837889.879632    4941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837889.879636    4941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764837889.879639    4941 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-04 08:44:49.883993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "ğŸ” Setting up authentication...\n",
            "âš ï¸ HuggingFace auth failed: 'NoneType' object has no attribute 'kernel'\n",
            "âœ… GPU detected: Tesla T4 (15.8 GB)\n",
            "\n",
            "ğŸ“‚ Loading test files...\n",
            "  âœ… Loaded 84 files\n",
            "\n",
            "âœ… Ready to test on 84 files and 20 concepts\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: nomic-ai/CodeRankEmbed\n",
            "############################################################\n",
            "  â†’ Using trust_remote_code=True\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "  â†’ Query prefix: 'Represent this query for searching relevant code: '\n",
            "  â†’ Document prefix: ''\n",
            "WARNING:transformers_modules.nomic_hyphen_ai.CodeRankEmbed.3c4b60807d71f79b43f3c4363786d9493691f8b1.modeling_hf_nomic_bert:<All keys matched successfully>\n",
            "\n",
            "============================================================\n",
            "Testing Model: nomic-ai/CodeRankEmbed\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "Batches:   0% 0/11 [00:00<?, ?it/s]\n",
            "  âŒ Error embedding files: CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 6.03 GiB is free. Process 47318 has 8.71 GiB memory in use. Of the allocated memory 8.44 GiB is allocated by PyTorch, and 149.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "ğŸ’¾ Results saved to results_nomic-ai_CodeRankEmbed.json\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: nomic-ai/nomic-embed-text-v1.5\n",
            "############################################################\n",
            "  â†’ Using trust_remote_code=True\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "  â†’ Query prefix: 'search_query: '\n",
            "  â†’ Document prefix: 'search_document: '\n",
            "WARNING:transformers_modules.nomic_hyphen_ai.nomic_hyphen_bert_hyphen_2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n",
            "\n",
            "============================================================\n",
            "Testing Model: nomic-ai/nomic-embed-text-v1.5\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "  â†’ Using document prefix: 'search_document: ' (applied to 84 docs)\n",
            "Batches: 100% 11/11 [00:04<00:00,  2.37it/s]\n",
            "  âœ… Embedded 84 files\n",
            "\n",
            "2ï¸âƒ£ Testing concepts...\n",
            "  âŒ context managers python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ async await python                       P@5: 0.40 | P@1: 1\n",
            "  âŒ decorators python                        P@5: 0.20 | P@1: 0\n",
            "  âŒ list comprehensions python               P@5: 0.40 | P@1: 0\n",
            "  âœ… exception handling python                P@5: 0.60 | P@1: 0\n",
            "  âŒ generators python                        P@5: 0.20 | P@1: 0\n",
            "  âŒ class inheritance python                 P@5: 0.40 | P@1: 1\n",
            "  âŒ file handling python                     P@5: 0.20 | P@1: 0\n",
            "  âŒ lambda functions python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ dataclasses python                       P@5: 0.00 | P@1: 0\n",
            "  âŒ promises javascript                      P@5: 0.00 | P@1: 0\n",
            "  âŒ async await javascript                   P@5: 0.00 | P@1: 0\n",
            "  âŒ react hooks                              P@5: 0.20 | P@1: 1\n",
            "  âŒ closures javascript                      P@5: 0.20 | P@1: 0\n",
            "  âœ… arrow functions javascript               P@5: 0.60 | P@1: 1\n",
            "  âŒ destructuring javascript                 P@5: 0.20 | P@1: 0\n",
            "  âŒ event handling javascript                P@5: 0.20 | P@1: 0\n",
            "  âŒ callbacks javascript                     P@5: 0.20 | P@1: 0\n",
            "  âœ… array methods javascript                 P@5: 0.60 | P@1: 0\n",
            "  âŒ classes javascript                       P@5: 0.20 | P@1: 0\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Overall Precision@5:  24.0%\n",
            "Overall Precision@1:  20.0%\n",
            "Mean Reciprocal Rank: 0.403\n",
            "Pass Rate (P@5â‰¥60%): 15.0% (3/20)\n",
            "Inference Time:       5.2s\n",
            "Peak GPU Memory:      9.28 GB\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Results saved to results_nomic-ai_nomic-embed-text-v1.5.json\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: intfloat/multilingual-e5-large-instruct\n",
            "############################################################\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "  â†’ Query prefix: 'query: '\n",
            "  â†’ Document prefix: 'passage: '\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\n",
            "============================================================\n",
            "Testing Model: intfloat/multilingual-e5-large-instruct\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "  â†’ Using document prefix: 'passage: ' (applied to 84 docs)\n",
            "Batches: 100% 11/11 [00:01<00:00,  6.03it/s]\n",
            "  âœ… Embedded 84 files\n",
            "\n",
            "2ï¸âƒ£ Testing concepts...\n",
            "  âŒ context managers python                  P@5: 0.00 | P@1: 0\n",
            "  âœ… async await python                       P@5: 0.60 | P@1: 1\n",
            "  âŒ decorators python                        P@5: 0.40 | P@1: 0\n",
            "  âŒ list comprehensions python               P@5: 0.20 | P@1: 0\n",
            "  âŒ exception handling python                P@5: 0.40 | P@1: 0\n",
            "  âŒ generators python                        P@5: 0.00 | P@1: 0\n",
            "  âœ… class inheritance python                 P@5: 1.00 | P@1: 1\n",
            "  âŒ file handling python                     P@5: 0.00 | P@1: 0\n",
            "  âŒ lambda functions python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ dataclasses python                       P@5: 0.00 | P@1: 0\n",
            "  âŒ promises javascript                      P@5: 0.40 | P@1: 0\n",
            "  âŒ async await javascript                   P@5: 0.20 | P@1: 1\n",
            "  âŒ react hooks                              P@5: 0.20 | P@1: 0\n",
            "  âŒ closures javascript                      P@5: 0.40 | P@1: 0\n",
            "  âŒ arrow functions javascript               P@5: 0.00 | P@1: 0\n",
            "  âŒ destructuring javascript                 P@5: 0.00 | P@1: 0\n",
            "  âŒ event handling javascript                P@5: 0.20 | P@1: 0\n",
            "  âŒ callbacks javascript                     P@5: 0.00 | P@1: 0\n",
            "  âœ… array methods javascript                 P@5: 0.60 | P@1: 1\n",
            "  âŒ classes javascript                       P@5: 0.00 | P@1: 0\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Overall Precision@5:  23.0%\n",
            "Overall Precision@1:  20.0%\n",
            "Mean Reciprocal Rank: 0.318\n",
            "Pass Rate (P@5â‰¥60%): 15.0% (3/20)\n",
            "Inference Time:       2.4s\n",
            "Peak GPU Memory:      9.28 GB\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Results saved to results_intfloat_multilingual-e5-large-instruct.json\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: Alibaba-NLP/gte-multilingual-base\n",
            "############################################################\n",
            "  â†’ Using trust_remote_code=True\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\n",
            "============================================================\n",
            "Testing Model: Alibaba-NLP/gte-multilingual-base\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "Batches: 100% 11/11 [00:04<00:00,  2.48it/s]\n",
            "  âœ… Embedded 84 files\n",
            "\n",
            "2ï¸âƒ£ Testing concepts...\n",
            "  âŒ context managers python                  P@5: 0.00 | P@1: 0\n",
            "  âœ… async await python                       P@5: 0.60 | P@1: 1\n",
            "  âŒ decorators python                        P@5: 0.00 | P@1: 0\n",
            "  âŒ list comprehensions python               P@5: 0.00 | P@1: 0\n",
            "  âœ… exception handling python                P@5: 0.60 | P@1: 0\n",
            "  âŒ generators python                        P@5: 0.20 | P@1: 0\n",
            "  âœ… class inheritance python                 P@5: 0.80 | P@1: 1\n",
            "  âŒ file handling python                     P@5: 0.20 | P@1: 0\n",
            "  âŒ lambda functions python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ dataclasses python                       P@5: 0.00 | P@1: 0\n",
            "  âŒ promises javascript                      P@5: 0.20 | P@1: 1\n",
            "  âŒ async await javascript                   P@5: 0.20 | P@1: 0\n",
            "  âŒ react hooks                              P@5: 0.40 | P@1: 1\n",
            "  âŒ closures javascript                      P@5: 0.20 | P@1: 0\n",
            "  âŒ arrow functions javascript               P@5: 0.20 | P@1: 0\n",
            "  âŒ destructuring javascript                 P@5: 0.00 | P@1: 0\n",
            "  âŒ event handling javascript                P@5: 0.40 | P@1: 1\n",
            "  âŒ callbacks javascript                     P@5: 0.20 | P@1: 0\n",
            "  âœ… array methods javascript                 P@5: 0.60 | P@1: 1\n",
            "  âŒ classes javascript                       P@5: 0.20 | P@1: 0\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Overall Precision@5:  25.0%\n",
            "Overall Precision@1:  30.0%\n",
            "Mean Reciprocal Rank: 0.458\n",
            "Pass Rate (P@5â‰¥60%): 20.0% (4/20)\n",
            "Inference Time:       4.8s\n",
            "Peak GPU Memory:      9.28 GB\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Results saved to results_Alibaba-NLP_gte-multilingual-base.json\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: Qwen/Qwen3-Embedding-0.6B\n",
            "############################################################\n",
            "  â†’ Using trust_remote_code=True\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "  â†’ Query prefix: 'query: '\n",
            "  â†’ Document prefix: ''\n",
            "\n",
            "============================================================\n",
            "Testing Model: Qwen/Qwen3-Embedding-0.6B\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "Batches: 100% 11/11 [00:16<00:00,  1.52s/it]\n",
            "  âœ… Embedded 84 files\n",
            "\n",
            "2ï¸âƒ£ Testing concepts...\n",
            "  âŒ context managers python                  P@5: 0.00 | P@1: 0\n",
            "  âœ… async await python                       P@5: 0.60 | P@1: 0\n",
            "  âŒ decorators python                        P@5: 0.40 | P@1: 0\n",
            "  âŒ list comprehensions python               P@5: 0.20 | P@1: 1\n",
            "  âŒ exception handling python                P@5: 0.40 | P@1: 1\n",
            "  âŒ generators python                        P@5: 0.20 | P@1: 0\n",
            "  âœ… class inheritance python                 P@5: 0.80 | P@1: 1\n",
            "  âŒ file handling python                     P@5: 0.20 | P@1: 1\n",
            "  âŒ lambda functions python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ dataclasses python                       P@5: 0.00 | P@1: 0\n",
            "  âŒ promises javascript                      P@5: 0.40 | P@1: 1\n",
            "  âŒ async await javascript                   P@5: 0.20 | P@1: 1\n",
            "  âŒ react hooks                              P@5: 0.20 | P@1: 1\n",
            "  âŒ closures javascript                      P@5: 0.20 | P@1: 1\n",
            "  âŒ arrow functions javascript               P@5: 0.20 | P@1: 0\n",
            "  âŒ destructuring javascript                 P@5: 0.00 | P@1: 0\n",
            "  âŒ event handling javascript                P@5: 0.20 | P@1: 0\n",
            "  âŒ callbacks javascript                     P@5: 0.00 | P@1: 0\n",
            "  âŒ array methods javascript                 P@5: 0.40 | P@1: 0\n",
            "  âŒ classes javascript                       P@5: 0.00 | P@1: 0\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Overall Precision@5:  23.0%\n",
            "Overall Precision@1:  40.0%\n",
            "Mean Reciprocal Rank: 0.525\n",
            "Pass Rate (P@5â‰¥60%): 10.0% (2/20)\n",
            "Inference Time:       17.9s\n",
            "Peak GPU Memory:      9.28 GB\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Results saved to results_Qwen_Qwen3-Embedding-0.6B.json\n",
            "\n",
            "\n",
            "############################################################\n",
            "Loading model: google/embeddinggemma-300m\n",
            "############################################################\n",
            "  â†’ Using torch.float16 for memory efficiency\n",
            "\n",
            "============================================================\n",
            "Testing Model: google/embeddinggemma-300m\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "1ï¸âƒ£ Embedding test files...\n",
            "Batches: 100% 11/11 [00:05<00:00,  1.85it/s]\n",
            "  âœ… Embedded 84 files\n",
            "\n",
            "2ï¸âƒ£ Testing concepts...\n",
            "  âŒ context managers python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ async await python                       P@5: 0.20 | P@1: 0\n",
            "  âŒ decorators python                        P@5: 0.00 | P@1: 0\n",
            "  âŒ list comprehensions python               P@5: 0.00 | P@1: 0\n",
            "  âœ… exception handling python                P@5: 0.60 | P@1: 1\n",
            "  âŒ generators python                        P@5: 0.00 | P@1: 0\n",
            "  âŒ class inheritance python                 P@5: 0.40 | P@1: 0\n",
            "  âŒ file handling python                     P@5: 0.20 | P@1: 0\n",
            "  âŒ lambda functions python                  P@5: 0.00 | P@1: 0\n",
            "  âŒ dataclasses python                       P@5: 0.00 | P@1: 0\n",
            "  âŒ promises javascript                      P@5: 0.00 | P@1: 0\n",
            "  âŒ async await javascript                   P@5: 0.00 | P@1: 0\n",
            "  âŒ react hooks                              P@5: 0.00 | P@1: 0\n",
            "  âŒ closures javascript                      P@5: 0.00 | P@1: 0\n",
            "  âŒ arrow functions javascript               P@5: 0.00 | P@1: 0\n",
            "  âŒ destructuring javascript                 P@5: 0.00 | P@1: 0\n",
            "  âŒ event handling javascript                P@5: 0.00 | P@1: 0\n",
            "  âŒ callbacks javascript                     P@5: 0.00 | P@1: 0\n",
            "  âŒ array methods javascript                 P@5: 0.00 | P@1: 0\n",
            "  âŒ classes javascript                       P@5: 0.00 | P@1: 0\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Overall Precision@5:  7.0%\n",
            "Overall Precision@1:  5.0%\n",
            "Mean Reciprocal Rank: 0.104\n",
            "Pass Rate (P@5â‰¥60%): 5.0% (1/20)\n",
            "Inference Time:       7.9s\n",
            "Peak GPU Memory:      9.28 GB\n",
            "============================================================\n",
            "\n",
            "ğŸ’¾ Results saved to results_google_embeddinggemma-300m.json\n",
            "\n",
            "\n",
            "\n",
            "## Model Comparison\n",
            "\n",
            "| Model | P@5 | P@1 | MRR | Pass Rate | Time (s) | Device | Decision |\n",
            "|-------|-----|-----|-----|-----------|----------|--------|----------|\n",
            "| Alibaba-NLP/gte-multilingual-base | 25.0% | 30.0% | 0.458 | 20.0% | 4.8 | cuda | âŒ NO-GO |\n",
            "| nomic-ai/nomic-embed-text-v1.5 | 24.0% | 20.0% | 0.403 | 15.0% | 5.2 | cuda | âŒ NO-GO |\n",
            "| Qwen/Qwen3-Embedding-0.6B | 23.0% | 40.0% | 0.525 | 10.0% | 17.9 | cuda | âŒ NO-GO |\n",
            "| intfloat/multilingual-e5-large-instruct | 23.0% | 20.0% | 0.318 | 15.0% | 2.4 | cuda | âŒ NO-GO |\n",
            "| google/embeddinggemma-300m | 7.0% | 5.0% | 0.104 | 5.0% | 7.9 | cuda | âŒ NO-GO |\n",
            "\n",
            "### Recommendation\n",
            "\n",
            "**âŒ STOP PROJECT** - Embedding approach not viable\n",
            "\n",
            "- Best model: `Alibaba-NLP/gte-multilingual-base` at 25.0% P@5\n",
            "- Threshold: â‰¥50% for conditional proceed\n",
            "- Recommendation: Consider alternative approaches or wait for better models\n",
            "\n",
            "\n",
            "ğŸ’¾ Comparison saved to comparison.md\n",
            "\n",
            "ğŸ’¾ Results saved to all_results.json\n",
            "\n",
            "ğŸ‰ Testing complete! Check comparison.md for decision.\n",
            "CPU times: user 214 ms, sys: 25.5 ms, total: 240 ms\n",
            "Wall time: 1min 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display comparison table\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "with open(\"comparison.md\", \"r\") as f:\n",
        "    display(Markdown(f.read()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "f_S_q9eaDc5D",
        "outputId": "342cdc8d-7e2f-4004-df62-d7860cb4f92d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Model Comparison\n\n| Model | P@5 | P@1 | MRR | Pass Rate | Time (s) | Device | Decision |\n|-------|-----|-----|-----|-----------|----------|--------|----------|\n| Alibaba-NLP/gte-multilingual-base | 25.0% | 30.0% | 0.458 | 20.0% | 4.8 | cuda | âŒ NO-GO |\n| nomic-ai/nomic-embed-text-v1.5 | 24.0% | 20.0% | 0.403 | 15.0% | 5.2 | cuda | âŒ NO-GO |\n| Qwen/Qwen3-Embedding-0.6B | 23.0% | 40.0% | 0.525 | 10.0% | 17.9 | cuda | âŒ NO-GO |\n| intfloat/multilingual-e5-large-instruct | 23.0% | 20.0% | 0.318 | 15.0% | 2.4 | cuda | âŒ NO-GO |\n| google/embeddinggemma-300m | 7.0% | 5.0% | 0.104 | 5.0% | 7.9 | cuda | âŒ NO-GO |\n\n### Recommendation\n\n**âŒ STOP PROJECT** - Embedding approach not viable\n\n- Best model: `Alibaba-NLP/gte-multilingual-base` at 25.0% P@5\n- Threshold: â‰¥50% for conditional proceed\n- Recommendation: Consider alternative approaches or wait for better models\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"all_results.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "models = [m for m in data[\"all_models\"] if \"error\" not in m]\n",
        "best = max(models, key=lambda x: x[\"overall_precision_at_5\"])\n",
        "\n",
        "print(f\"ğŸ† Best Model: {best['model']}\")\n",
        "print(f\"   P@5: {best['overall_precision_at_5']:.1%}\")\n",
        "print(f\"   P@1: {best['overall_precision_at_1']:.1%}\")\n",
        "print(f\"   Device: {best['device']}\")\n",
        "\n",
        "# Failed concepts\n",
        "failed = [c for c in best['per_concept'] if c['precision_at_5'] < 0.6]\n",
        "if failed:\n",
        "    print(f\"\\nâŒ Failed Concepts ({len(failed)}):\")\n",
        "    for c in failed[:5]:\n",
        "        print(f\"   - {c['concept']}\")\n",
        "\n",
        "# Top concepts\n",
        "top = sorted(best['per_concept'], key=lambda x: x['precision_at_5'], reverse=True)[:5]\n",
        "print(f\"\\nâœ… Top Concepts:\")\n",
        "for c in top:\n",
        "    print(f\"   - {c['concept']}: {c['precision_at_5']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhA4rSxHDhiM",
        "outputId": "f0094672-45a0-4682-c5d0-a3efede72088"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ† Best Model: Alibaba-NLP/gte-multilingual-base\n",
            "   P@5: 25.0%\n",
            "   P@1: 30.0%\n",
            "   Device: cuda\n",
            "\n",
            "âŒ Failed Concepts (16):\n",
            "   - context managers python\n",
            "   - decorators python\n",
            "   - list comprehensions python\n",
            "   - generators python\n",
            "   - file handling python\n",
            "\n",
            "âœ… Top Concepts:\n",
            "   - class inheritance python: 0.80\n",
            "   - async await python: 0.60\n",
            "   - exception handling python: 0.60\n",
            "   - array methods javascript: 0.60\n",
            "   - react hooks: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped backup\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup_dir = Path(f\"/content/drive/MyDrive/phase1_results_{timestamp}\")\n",
        "backup_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy results\n",
        "files_to_backup = [\n",
        "    \"all_results.json\",\n",
        "    \"comparison.md\",\n",
        "    \"test_code/metadata.json\"\n",
        "]\n",
        "\n",
        "for f in files_to_backup:\n",
        "    if Path(f).exists():\n",
        "        shutil.copy(f, backup_dir / Path(f).name)\n",
        "\n",
        "# Copy per-model results\n",
        "for f in Path(\".\").glob(\"results_*.json\"):\n",
        "    shutil.copy(f, backup_dir / f.name)\n",
        "\n",
        "print(f\"âœ… Backed up to: {backup_dir}\")\n",
        "print(f\"\\nğŸ“‚ Files saved:\")\n",
        "for f in backup_dir.iterdir():\n",
        "    print(f\"   - {f.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwgcmHLIDkGs",
        "outputId": "11699f37-7370-477e-8f93-33a124e40e2e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Backed up to: /content/drive/MyDrive/phase1_results_20251204_084619\n",
            "\n",
            "ğŸ“‚ Files saved:\n",
            "   - all_results.json\n",
            "   - comparison.md\n",
            "   - metadata.json\n",
            "   - results_intfloat_multilingual-e5-large-instruct.json\n",
            "   - results_Alibaba-NLP_gte-multilingual-base.json\n",
            "   - results_nomic-ai_CodeRankEmbed.json\n",
            "   - results_BAAI_bge-small-en-v1.5.json\n",
            "   - results_Qwen_Qwen3-Embedding-0.6B.json\n",
            "   - results_nomic-ai_nomic-embed-text-v1.5.json\n",
            "   - results_google_embeddinggemma-300m.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save decision for Phase 2\n",
        "with open(\"phase1_decision.txt\", \"w\") as f:\n",
        "    f.write(f\"DECISION: GO TO PHASE 2\\n\")\n",
        "    f.write(f\"Model: {best['model']}\\n\")\n",
        "    f.write(f\"P@5: {best['overall_precision_at_5']:.1%}\\n\")\n",
        "\n",
        "print(\"âœ… Decision saved to phase1_decision.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7rgpkaKDnus",
        "outputId": "fe08dc28-43fc-4564-e24d-2f863a934c11"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Decision saved to phase1_decision.txt\n"
          ]
        }
      ]
    }
  ]
}